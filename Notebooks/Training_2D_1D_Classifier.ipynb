{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f168f8b",
   "metadata": {},
   "source": [
    "# Training Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d968dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "seed = 786\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "# tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, UpSampling1D, Concatenate, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "\n",
    "\n",
    "defnormalize_array(array):\n",
    "    normalized_array = np.zeros_like(array)  # Create a new array to store normalized values\n",
    "    scale_fac = []\n",
    "    for i in range(array.shape[0]):  # Iterate over samples/events\n",
    "        max_value = np.max(np.abs(array[i]))  # Calculate the maximum value across both channels\n",
    "        \n",
    "        if max_value != 0:  # Avoid division by zero\n",
    "            normalized_sample = array[i] / max_value\n",
    "            normalized_array[i] = normalized_sample\n",
    "        else:\n",
    "            print(\"Max vlue 0 encountered at index :\", i)\n",
    "        scale_fac.append(max_value)\n",
    "    return normalized_array, scale_fac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e4d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ant = \"ant1\"\n",
    "\n",
    "DataDir = (\"/mnt/janus/arehman/work/2023_Split_months_data-Final/JanFeb/2D_CNN/Classifier/\")\n",
    "\n",
    "x_train = np.load(DataDir + f\"/TstTrn/{ant}_Traces_train.npy\")\n",
    "x_test = np.load(DataDir + f\"/TstTrn/{ant}_Traces_test.npy\")\n",
    "\n",
    "y_train = np.load(DataDir + f\"/TstTrn/{ant}_Labels_train.npy\")\n",
    "y_test = np.load(DataDir + f\"/TstTrn/{ant}_Labels_test.npy\")\n",
    "\n",
    "\n",
    "## Convert to 1D for single channel training\n",
    "# ch0_train, ch1_train = x_train[:,:,0], x_train[:,:,1]\n",
    "# ch0_test, ch1_test = x_test[:,:,0], x_test[:,:,1]\n",
    "\n",
    "# x_train = np.concatenate((ch0_train, ch1_train))\n",
    "# x_test = np.concatenate((ch0_test, ch1_test))\n",
    "\n",
    "# y_train = np.concatenate((y_train, y_train))\n",
    "# y_test = np.concatenate((y_test, y_test))\n",
    "\n",
    "\n",
    "### Normalizing\n",
    "x_train, scalefac = normalize_array(x_train)\n",
    "x_test, scalefacT = normalize_array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e87556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train =  np.array(x_train, dtype='float32')\n",
    "# print(f\"Total size in Mbs = {x_train.nbytes/10**9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6fd569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(Callback):\n",
    "    def __init__(self, factor, patience):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_val_loss = logs.get('val_loss')\n",
    "        \n",
    "        if current_val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = current_val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                old_lr = float(K.get_value(self.model.optimizer.lr))\n",
    "                new_lr = old_lr * self.factor\n",
    "                K.set_value(self.model.optimizer.lr, new_lr)\n",
    "                self.wait = 0\n",
    "                print(f\"Reducing learning rate to {new_lr}\")\n",
    "\n",
    "                \n",
    "def Classifier(KS, FIL, Layers, lr=5e-3):\n",
    "    inputs = Input(shape=(1000, 2))\n",
    "    x = Conv1D(filters= FIL, kernel_size=KS, padding='same', activation='relu', kernel_initializer='he_normal') (inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Conv1D(filters= FIL, kernel_size=KS, padding='same', activation='relu', kernel_initializer='he_normal') (x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Conv1D(filters= FIL, kernel_size=KS, padding='same', activation='relu', kernel_initializer='he_normal') (x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    ## Decode\n",
    "    x = Conv1D(filters= FIL, kernel_size=KS, padding='same', activation='relu', kernel_initializer='he_normal') (x)\n",
    "    x = UpSampling1D(2)(x)\n",
    "    x = Conv1D(filters= FIL, kernel_size=KS, padding='same', activation='relu', kernel_initializer='he_normal') (x)\n",
    "    x = UpSampling1D(2)(x)\n",
    "    x = Conv1D(filters= FIL, kernel_size=KS, padding='same', activation='relu', kernel_initializer='he_normal') (x)\n",
    "    x = UpSampling1D(2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, output)\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='bce', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    \n",
    "    return model\n",
    "                \n",
    "def CreateModel(KS, FIL, Layers, lr=5e-3):\n",
    "\n",
    "    # Define the input shape for each polarization\n",
    "#     input_shape = (1000, 2)  # 2 channels, 1 for each polarization\n",
    "    input_shape = (1000, 1)  # 2 channels, 1 for each polarization\n",
    "    \n",
    "\n",
    "    # Define the input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = input_layer\n",
    "\n",
    "    # Convolutional layers\n",
    "    for i in range(Layers):\n",
    "        x = Conv1D(filters=FIL, kernel_size=KS, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # Flatten the pooled output\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "#     x = Dense(64, activation='relu')(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mse', metrics=['accuracy', f1_score, precision, recall_m])\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='bce', metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mse', metrics=[tf.keras.metrics.Recall(), recall_m])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9328d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelsDir = \"./1D_models\"\n",
    "ModelsDir = DataDir + \"/Models\"\n",
    "### Fitting (Training the data)\n",
    "lr_scheduler = LearningRateScheduler(factor=0.5, patience=5)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min',restore_best_weights=True, verbose=1, patience=5)\n",
    "mc = ModelCheckpoint(ModelsDir+f'/Classifier_{ant}.h5',\n",
    "                                    monitor = 'val_loss', mode = 'min', verbose=1, save_best_only=True)\n",
    "\n",
    "autoencoder = Classifier(KS=33, FIL=12, Layers=3, lr=1e-4)\n",
    "\n",
    "history = autoencoder.fit(x_train, y_train,\n",
    "                          epochs=200,\n",
    "                          batch_size=300,\n",
    "                          shuffle=True,\n",
    "                          validation_data=(x_test, y_test),\n",
    "                          callbacks=[lr_scheduler, es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaafac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1dde13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pre = history.history[list(history.history.keys())[2]]\n",
    "test_pre = history.history[list(history.history.keys())[6]]\n",
    "\n",
    "train_recall = history.history[list(history.history.keys())[3]]\n",
    "test_recall = history.history[list(history.history.keys())[7]]\n",
    "\n",
    "train_loss = history.history[list(history.history.keys())[0]]\n",
    "test_loss = history.history[list(history.history.keys())[4]]\n",
    "\n",
    "train_accu = history.history[list(history.history.keys())[1]]\n",
    "test_accu = history.history[list(history.history.keys())[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9265a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label=\"train_loss\")\n",
    "plt.plot(test_loss, label=\"test_loss\")\n",
    "plt.figure()\n",
    "plt.plot(train_accu, label=\"train_accu\")\n",
    "plt.plot(test_accu, label=\"test_accu\")\n",
    "plt.figure()\n",
    "plt.plot(train_pre, label=\"train_pre\")\n",
    "plt.plot(test_pre, label=\"test_pre\")\n",
    "\n",
    "plt.plot(train_recall, label=\"train_recall\")\n",
    "plt.plot(test_recall, label=\"test_recall\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1102bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f4823fd",
   "metadata": {},
   "source": [
    "# Training Denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "seed = 786\n",
    "tf.keras.utils.set_random_seed(seed)\n",
    "# tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, UpSampling1D, Concatenate, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "\n",
    "\n",
    "def GetChunkSNR(trace, No_Chunks):\n",
    "    Amps = np.array(trace)\n",
    "\n",
    "    Trace_Peak = np.max(np.abs(Amps))  ## to save computation time, otherwise hilberenvelop can also be used\n",
    "    Chunks = np.array_split(Amps, No_Chunks)\n",
    "    ChunkRMS_squared = [(sum(chunk ** 2)) / len(chunk) for chunk in Chunks]\n",
    "    RMS_squared_Median = np.median(ChunkRMS_squared)\n",
    "    SNR = Trace_Peak ** 2 / RMS_squared_Median\n",
    "\n",
    "    return SNR\n",
    "\n",
    "\n",
    "def GettrueSNR(trace, label, No_Chunks=10):\n",
    "    Amps = np.array(trace)\n",
    "\n",
    "    Trace_Peak = np.max(np.abs(label))  ## to save computation time, otherwise hilberenvelop can also be used\n",
    "#     Trace_Peak = np.max(np.abs(Amps))  ## to save computation time, otherwise hilberenvelop can also be used\n",
    "    Chunks = np.array_split(Amps, No_Chunks)\n",
    "    ChunkRMS_squared = [(sum(chunk ** 2)) / len(chunk) for chunk in Chunks]\n",
    "    RMS_squared_Median = np.median(ChunkRMS_squared)\n",
    "    SNR = Trace_Peak ** 2 / RMS_squared_Median\n",
    "\n",
    "    return SNR\n",
    "\n",
    "def normalize_deno_array(array, y_array):\n",
    "    normalized_array = np.zeros_like(array)  # Create a new array to store normalized values\n",
    "    normalized_y_array = np.zeros_like(array)  # Create a new array to store normalized values\n",
    "    \n",
    "    scale_fac = []\n",
    "    for i in range(array.shape[0]):  # Iterate over samples/events\n",
    "        max_value = np.max(np.abs(array[i]))  # Calculate the maximum value across both channels\n",
    "        \n",
    "        if max_value != 0:  # Avoid division by zero\n",
    "            normalized_sample = array[i] / max_value\n",
    "            normalized_array[i] = normalized_sample\n",
    "            \n",
    "            ## Normalizing labels\n",
    "            normalized_y_sample = y_array[i] / max_value\n",
    "            normalized_y_array[i] = normalized_y_sample\n",
    "            \n",
    "        else:\n",
    "            print(\"Max vlue 0 encountered at index :\", i)\n",
    "        scale_fac.append(max_value)\n",
    "    return normalized_array, normalized_y_array, scale_fac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d8405",
   "metadata": {},
   "outputs": [],
   "source": [
    "ant = \"ant3\"\n",
    "\n",
    "DataDir = (\"/mnt/janus/arehman/work/2023_Split_months_data/MayJune/2D_CNN/Denoiser/TstTrn\")\n",
    "\n",
    "x_train = np.load(DataDir + f\"/{ant}_Noisy_train.npy\")\n",
    "x_test = np.load(DataDir + f\"/{ant}_Noisy_test.npy\")\n",
    "\n",
    "y_train = np.load(DataDir + f\"/{ant}_Signals_train.npy\")\n",
    "y_test = np.load(DataDir + f\"/{ant}_Signals_test.npy\")\n",
    "\n",
    "\n",
    "# ########\n",
    "# ## For Single channel \n",
    "# train_pol0 = [x_train[:, :, 0][i] for i in range(len(x_train))]\n",
    "# train_pol1 = [x_train[:, :, 1][i] for i in range(len(x_train))]\n",
    "\n",
    "# test_pol0 = [x_test[:, :, 0][i] for i in range(len(x_test))]\n",
    "# test_pol1 = [x_test[:, :, 1][i] for i in range(len(x_test))]\n",
    "\n",
    "\n",
    "# train_lab_pol0 = [y_train[:, :, 0][i] for i in range(len(y_train))]\n",
    "# train_lab_pol1 = [y_train[:, :, 1][i] for i in range(len(y_train))]\n",
    "\n",
    "# test_lab_pol0 = [y_test[:, :, 0][i] for i in range(len(y_test))]\n",
    "# test_lab_pol1 = [y_test[:, :, 1][i] for i in range(len(y_test))]\n",
    "\n",
    "# x_train = np.concatenate((train_pol0, train_pol1))\n",
    "# x_test = np.concatenate((test_pol0, test_pol1))\n",
    "\n",
    "# y_train = np.concatenate((train_lab_pol0, train_lab_pol1))\n",
    "# y_test = np.concatenate((test_lab_pol0, test_lab_pol1))\n",
    "\n",
    "\n",
    "### Normalizing\n",
    "x_train, y_train, scalefac = normalize_deno_array(x_train, y_train)\n",
    "x_test, y_test, scalefacT = normalize_deno_array(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc75272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(Callback):\n",
    "    def __init__(self, factor, patience):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_val_loss = logs.get('val_loss')\n",
    "        \n",
    "        if current_val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = current_val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                old_lr = float(K.get_value(self.model.optimizer.lr))\n",
    "                new_lr = old_lr * self.factor\n",
    "                K.set_value(self.model.optimizer.lr, new_lr)\n",
    "                self.wait = 0\n",
    "                print(f\"Reducing learning rate to {new_lr}\")\n",
    "\n",
    "def CorrCoeff(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = K.mean(x)\n",
    "    my = mx\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = K.sum(tf.multiply(xm,ym))\n",
    "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
    "    r = r_num / r_den\n",
    "    return r\n",
    "# Custom correlation metric\n",
    "def CC2(y_true, y_pred):\n",
    "    def pearson_correlation(x, y):\n",
    "        mean_x = K.mean(x)\n",
    "        mean_y = K.mean(y)\n",
    "        cov_xy = K.sum((x - mean_x) * (y - mean_y))\n",
    "        std_x = K.sqrt(K.sum(K.square(x - mean_x)))\n",
    "        std_y = K.sqrt(K.sum(K.square(y - mean_y)))\n",
    "        correlation = cov_xy / (std_x * std_y)\n",
    "        return correlation\n",
    "\n",
    "    correlation = tf.py_function(pearson_correlation, [y_true, y_pred], tf.float32)\n",
    "    return correlation\n",
    "\n",
    "\n",
    "def Denoiser(fil, ks, lr=1e-3):\n",
    "#     input_shape = (1000, 2)\n",
    "    input_shape = (1000, 1)\n",
    "\n",
    "    # Encoder architecture\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "#     fil, ks = 4, 20\n",
    "    x = Conv1D(fil, ks, activation='relu', padding='same')(input_layer)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Conv1D(fil, ks, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "    x = Conv1D(fil, ks, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(2, padding='same')(x)\n",
    "#     x = Conv1D(fil, ks, activation='relu', padding='same')(x)\n",
    "#     x = MaxPooling1D(2)(x)\n",
    "\n",
    "# #     x = Conv1D(fil, ks, activation='relu', padding='same')(x)\n",
    "#     # Decoder architecture\n",
    "#     x = Conv1D(fil, ks, activation='relu', padding='same')(x)\n",
    "#     x = UpSampling1D(2)(x)\n",
    "    x = Conv1D(fil, ks, activation='relu', padding='same')(x)\n",
    "    x = UpSampling1D(2)(x)\n",
    "    x = Conv1D(fil, ks, activation='relu', padding='same')(x)\n",
    "    x = UpSampling1D(2)(x)\n",
    "    x = Conv1D(fil, ks, activation='relu', padding='same')(x)\n",
    "    x = UpSampling1D(2)(x)\n",
    "#     decoded = Conv1D(2, ks, activation='linear', padding='same')(x)\n",
    "    decoded = Conv1D(1, ks, activation='linear', padding='same')(x)\n",
    "    \n",
    "\n",
    "    # Create the autoencoder model\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "    # Compile the model\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=[CC2])\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf17622",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelsDir = \"./1D_models\"\n",
    "### Fitting (Training the data)\n",
    "lr_scheduler = LearningRateScheduler(factor=0.5, patience=5)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min',restore_best_weights=True, verbose=1, patience=5)\n",
    "mc = ModelCheckpoint(ModelsDir+f'/Denoiser_{ant}.h5',\n",
    "                                    monitor = 'val_loss', mode = 'min', verbose=1, save_best_only=True)\n",
    "\n",
    "autoencoder = Denoiser(fil=12, ks=33, lr=1e-4)\n",
    "\n",
    "# x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "\n",
    "history = autoencoder.fit(x_train, y_train, \n",
    "                          epochs=500, \n",
    "                          batch_size=200, shuffle=True,\n",
    "                          validation_data=(x_test, y_test), \n",
    "                          callbacks=[lr_scheduler, es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216d3c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c475c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe05f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tf-gpu-2.7.0] *",
   "language": "python",
   "name": "conda-env-.conda-tf-gpu-2.7.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
